\documentclass[12pt]{article}  
\usepackage{graphicx}
\usepackage{geometry}   %设置页边距的宏包
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{kpfonts}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{color}
\usepackage{pgf,tikz}
\usepackage{mathtools}
\usetikzlibrary{automata, positioning, arrows}
\usepackage{wrapfig}
\newcommand{\solu}{{\color{blue} Solution:}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Lap}{\mathcal{L}}
\newcommand{\Zplus}{\mathbf{Z}^+}
\newcommand{\indep}{\perp \!\!\! \perp}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}  %设置 上、左、下、右 页边距

\title{DS4400 HW3}
\author{Xin Guan}
\date{}


\begin{document}
    \maketitle
    \begin{enumerate}
        \item \textbf{MAP estimation.} Consider a Bernoulli random variable x with $p(x = 1) = \theta$. Given a dataset $D = \{x_1, \dots ,x_N\}$, assume $N_1$ is the number of trials where $x_i = 1$, $N_0$ is the number of trials where $x_i = 0$ and $N = N_0 + N_1$ is the total number of trials. Consider the following prior, that believes the experiments is biased:
        $$p(\theta) = \left\{
            \begin{array}{rcl}
            0.2  & & \text{if }\theta = 0.6 \\
            0.8 & & \text{if }\theta = 0.8 \\
            0 & & \text{otherwise}
        \end{array}
        \right.$$
        \begin{enumerate}
            \item Write down the likelihood function, i.e., $p(D|\theta)$. What is the maximum likelihood solution for $\theta$ (we already have derived this in the class)?
            
            \solu
            
            we write $P(X = x) = \theta^x(1-\theta)^{(1-x)}$\\
            $P(D | \theta) = P(X_1 = x_1, X_2 = x_2 \dots, X_N= x_N)\\
            = \prod\limits_{x = 1}^{N}\theta^x_i(1-\theta)^{(1-x_i)}\\
            = \theta^{N_1}(1-\theta)^{N_0}$\\
            Let $J(\theta) = logP(D | \theta) = N_1log(\theta) + N_0log(1 - \theta)$\\
            $\frac{\partial J(\theta)}{\partial \theta} = \frac{N_1}{\theta} - \frac{N_0}{1 - \theta}$.
            Let $\frac{\partial J(\theta)}{\partial \theta} = 0$.\\
            Then $\hat{\theta} = \frac{N_1}{N_0 + N_1} = \frac{N_1}{N}$\\
            Therefore, the maximum likelihood solution is $\hat{\theta} = \frac{N_1}{N}$

            \item Consider maximizing the posterior distribution, $p(D|\theta) \times p(\theta)$, that takes advantage of the prior. What is the MAP estimation?
            
            \solu 
            
            $p(D | \theta) \times p(\theta) =  \left\{
                \begin{array}{rcl}
                0.2\cdot 0.6^{N_1} \cdot 0.4^{N_0}  & & \theta = 0.6 \\
                0.8\cdot 0.8^{N_1} \cdot 0.2^{N_0} & & \theta = 0.8 \\
                0 & & \text{otherwise}
            \end{array}
            \right.$\\
            $\frac{P(D|0.6)P(0.6)}{P(D|0.8)P(0.8)} = \frac{1}{4}(\frac{3}{4})^{N_1}(2)^{N_0} = 3^{N_1}4^{-N_1-1}2^{N_0} = 2^{N_1log_23}2^{-2N_1 - 2}2^{N_0} = 2^{N_0 - (2-log_23)N_1 - 2}$\\
            Therefore, when $\frac{P(D|0.6)P(0.6)}{P(D|0.8)P(0.8)} \ge 1:$\\
            $N_0 -(2-log_23)N_1 -2 \ge 0 \Rightarrow N_0 \ge (2-log_23)N_1 +2$\\
            Therefore, $\hat{\theta} = \left\{
                \begin{array}{rcl}
                0.6 & & N_0 \ge (2-log_23)N_1 +2 \\
                0.8 & & N_0 < (2-log_23)N_1 +2
            \end{array}
            \right.$
        \end{enumerate}

        \item \textbf{Naive Bayes Classifier.} Assume you have the following training set with two binary features $x_1$ and $x_2$, and a binary response/output $y$. Suppose you have to predict $y$ using a naive Bayes classifier.
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline 
                $x_1$ & $x_2$ & $y$\\ \hline 
                1 & 0 & 0\\ \hline 
                0 & 1 & 0\\ \hline 
                0 & 0 & 0\\ \hline 
                1 & 0 & 1\\ \hline 
                0 & 0 & 1\\ \hline 
                0 & 1 & 1\\ \hline 
                1 & 1 & 1 \\ \hline 
            \end{tabular}
        \end{center}
        
        \begin{enumerate}
            \item Compute the Maximum Likelihood Estimates (MLE) for $\theta^y_j$ for $j = 0, 1$ as well as $\theta^{{x_l}|y}_{{\overbar{x}_l}|y}$ for $j = 0,1$ and for $\ell=1,2$.
            
            \solu 
        
            $\theta^y_j = \left\{
                \begin{array}{rcl}
                    \frac{3}{7} & & j = 0\\
                    \frac{4}{7} & & j = 1
                \end{array}
            \right.$\\
            $\theta^{x_\ell | y}_{\overbar{x}_\ell | j} = \left\{
                \begin{array}{rcl}
                    \frac{1}{3} & & x_1 = 1, j = 0\\
                    \frac{2}{3} & & x_1 = 0, j = 0\\
                    \frac{1}{3} & & x_2 = 1, j = 0\\
                    \frac{2}{3} & & x_2 = 0, j = 0\\
                    \frac{1}{2} & & x_1 = 1, j = 1\\
                    \frac{1}{2} & & x_1 = 0, j = 1\\
                    \frac{1}{2} & & x_2 = 1, j = 1\\
                    \frac{1}{2} & & x_2 = 0, j = 1
                \end{array}
            \right.$

            \item After learning via MLE is complete, what would be the estimate for $P (y = 0|x1 = 0, x2 = 1)$.
            
            \solu 
            
            $P(y = 0 | x_1 = 0, x_2 = 1)\\
            = \frac{P(x_1 = 0, x_2 = 1 | y = 0)P(y = 0)}{P(x_1 = 0, x_2 = 1 | y= 0)P(y = 0) + P(x_1 = 0, x_2 = 1 |y = 1)P(y = 1)}\\
            = \frac{P(x_1 = 0| y = 0)P(x_2 = 1| y = 0)P(y = 0)}{P(x_1 = 0| y= 0)P(x_2 = 1| y= 0)P(y = 0) + P(x_1 = 0| y= 1)P(x_2 = 1| y= 1)P(y = 1)}\\
            = \frac{\theta^{x_1 | y}_{0 | 0}\theta^{x_2 | y}_{1 | 0}\theta^y_0}{\theta^{x_1 | y}_{0 | 0}\theta^{x_2 | y}_{1 | 0}\theta^y_0 + \theta^{x_1 | y}_{0 | 1}\theta^{x_2 | y}_{1 | 1}\theta^y_1}\\
            = \frac{\frac{2}{3} \cdot \frac{1}{3} \cdot \frac{3}{7}}{\frac{2}{3} \cdot \frac{1}{3} \cdot \frac{3}{7} + \frac{2}{4} \cdot \frac{2}{4} \cdot \frac{4}{7}}\\
            = \frac{2}{5}$

            \item What would be the solution of the previous part without the naive Bayes assumption?
            
            \solu 
            
            Without Naive Bayes Assumption:\\
            $P(y = 0 | x_1 = 0, x_2 = 1)\\
            =P(y = 0, x_1 = 0, x_2 = 1) / P(x_1 = 0, x_2 = 1)\\
            =\frac{1}{2}$
        \end{enumerate}

        \item Constrained Optimization. Consider the regression problem on a dataset $\{(x_i, y_i)\}N_i=1$, where
        $x_i \in \mathbb{R}^k$ denotes the input and $y_i$ denotes the output/response. Let $\mathbf{X} = [\mathbf{x}_1 \ \dots \ \mathbf{x}_N]^T$ and 
        $\mathbf{y} = [\mathbf{y}_1 \ \dots \ \mathbf{y}_N]^T$ . Consider the following regression optimization.
        $$\min_\theta \frac{1}{2}||\mathbf{y} - \mathbf{X\theta}||^2_2$$
        $$\text{ s.t. } \mathbf{\omega^T\theta} = \mathbf{b}$$
        where $\omega$ and $b$ are given and indicate the parameters of a hyperplane on which the desired parameter vector, $\theta$, lies.
        \begin{enumerate}
            \item Assume that $X^TX = I^k$, where $I_k$ denotes the identity matrix. Find the closed-form solution of the above regression problem.
            
            \solu

            Let $L(\theta, \alpha) = \frac{1}{2}||\mathbf{y} - \mathbf{X\theta}||^2_2 + \alpha(\omega^T\theta - b)$\\
            Then:\\
            $\frac{\partial L(\theta, \alpha)}{\partial \theta} = \frac{1}{2} \frac{\partial||\mathbf{y} - \mathbf{X\theta}||^2_2}{\partial \theta}  + \alpha \frac{\partial \omega^T \theta - b}{\partial \theta}\\
            =\frac{1}{2} \frac{\partial||\begin{bmatrix}
                y_1 - x_1\theta \\
                y_2 - x_2\theta \\
                \dots \\
                y_N - x_N\theta \\
            \end{bmatrix}||^2_2}{\partial \theta} + \alpha \omega\\
            = \frac{1}{2}\frac{\partial \sum_{i = 1}^{N}(y_i - x_i\theta)^2}{\partial \theta} + \alpha \omega \\
            = \frac{1}{2} \sum_{i = 1}^{N}2(y_i - x_i\theta)(-x_i) + \alpha \omega\\
            = \sum_{i = 1}^{N}(-x_i y_i + (x_i)^Tx_i\theta)(-x_i) + \alpha \omega\\
            = -X^T y+ X^TX\theta + \alpha \omega\\
            = -X^T y+ \theta + \alpha \omega$\\
            $\frac{\partial L(\theta, \alpha)}{\partial \alpha} = \omega^T\theta - b $.\\
            Let $
            \left\{
                \begin{array}{l}
                    \frac{\partial L(\theta, \alpha)}{\partial \theta} =0 \\
                    \frac{\partial L(\theta, \alpha)}{\partial \alpha} = 0
                \end{array}
            \right.$
            Then, $
            \left\{
                \begin{array}{l}
                    -X^T y+ \theta + \alpha \omega =0 \\
                    \omega^T\theta - b  = 0
                \end{array}
            \right.$\\
            Since, $\theta = X^Ty - \alpha \omega$,\\
            then $\omega^T (X^Ty - \alpha \omega) = b$.\\
            $\Rightarrow \omega^TX^T y - \omega^T\alpha\omega - b = 0$.\\
            $\Rightarrow \omega^TX^T y - b = \omega^T\omega \alpha\\
            \Rightarrow \alpha = \frac{\omega^TX^T y - b}{\omega^T \omega}$.\\
            Therefore $\theta^* = X^Ty - \frac{\omega^TX^T y - b}{\omega^T \omega} \omega$

            \item Verify if your obtained solution $\theta^*$ satisfies the constraint $\omega^T\theta^* = b$.
            
            \solu

            $\omega^T \theta^* \\
            = \omega^T(X^Ty - \frac{\omega^TX^T y - b}{\omega^T \omega} \omega)\\
            = \omega^TX^Ty - \frac{\omega^TX^T y - b}{\omega^T \omega} \omega^T \omega\\
            = \omega^TX^Ty - (\omega^TX^T y - b)\\
            = b$

            \item What you have been the solution of this optimization, if the constraint $\omega^T\theta = b$ was not present?
            
            \solu 

            If there is no constraint $\omega^T\theta = b$, it is just $\min\limits_\theta \frac{1}{2}||\mathbf{y} - \mathbf{X\theta}||^2_2$.\\
            Then $\frac{\partial \frac{1}{2}||\mathbf{y} - \mathbf{X\theta}||^2_2}{\partial \theta} = X^TX\theta - X^Ty$. Let it to be 0.\\
            Then $\theta^* = (X^TX)^{-1}X^Ty$\\
            If we still preserve the assumption in problem $a$ where $X^TX = I^k$, $\theta^* = X^Ty$


            

        \end{enumerate}
    \end{enumerate}    
\end{document}
