\documentclass[12pt]{article}  
\usepackage{graphicx}
\usepackage{geometry}   %设置页边距的宏包
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{kpfonts}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{color}
\newcommand{\solu}{{\color{blue} Solution:}}
\newcommand{\theo}{{\color{blue} Theorem: $\ $}}
\newcommand{\defi}{{\color{blue} Definition: $\ $}}
\newcommand{\recall}{{\color{blue} Recall: $\ $}}
\newcommand{\exe}{{\color{green} Exercise: $\ $}}
\newcommand{\prop}{{\color{blue} Prop: $\ $}}

\newcommand{\hw}{{\color{red} Homework: $\ $}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Lap}{\mathcal{L}}
\newcommand{\Zplus}{\mathbf{Z}^+}
\geometry{left=2cm,right=2cm,top=1.5cm,bottom=2cm}  %设置 上、左、下、右 页边距

\title{DS4400 Notes}
\date{}
\author{}

\begin{document}
    \maketitle

    \begin{multicols}{2}
        \title{DS4400 Notes 01/24}
        \maketitle
        \begin{enumerate}
            \item Convex functions:
            
            A function $f:\mathbb{R}^d \rightarrow \mathbb{R}$ is convex iff $\forall \theta_1, \theta_2 \in \mathbb{R}^d $ and $\forall \alpha \in [0,1]$ we have 
            $f(\alpha\theta_1 + (1-\alpha)\theta_2) \le \alpha f(\theta_1) + (1-\alpha)f(\theta_2)$
    
            In the special case $(d=1) \ f:\mathbb{R}\rightarrow \mathbb{{R}}$, $f$ is convex iff $\forall \theta, f''(\theta) \ge 0$

            When the function is convex, \textbf{local min} $\equiv$ \textbf{global min}. When the system is not convex, we might find only a \textbf{local min} but not a \textbf{global min}

            \item Dealing with non convex function:
            
            In gradient descent: 
            \begin{enumerate}
                \item use larger $\rho$ in the beginning and gradually decrease $\rho$ with interation.
                \item Run SGD/GD with multiple random initializaitons $\theta_1^{(0)}, \theta_2^{(0)} \dots $ and keep the best solution.
            \end{enumerate}

            \item $\min_{\theta} \sum_{i = 1}^N (y_i - \theta^Tx_i)^2 \triangleq J(\theta)$ 
            
            In linear regression, $J(\theta)$ is convex.

            \item Robustness of Regression to outliers:
            
            \begin{enumerate}
                \item Run outlier detection algorithm, remove detected outliers, then run Linear Regression on remaining points.
                \item Robust Regression cost function.
                
                $\min_{\theta} \sum_{i = 1}^{N} e_i^2, \ e_i \triangleq y_i - \theta^T x_i$

                $e^2$ is extremly unhappy with large errors.

                we might use $|e|$ to replace the function. This might be more tolerance. Then, $\min_{\theta} \sum_{i = 1}^N |y_i - \theta^T x_i|$
            \end{enumerate}
            
            \item \exe D = \{$(x_1, y_1 = 100) \dots (x_10, y_10 = 100), (x_{11}, y_{11} = 0),  (x_{12}, y_{12} = 0)$\}
            
            $e^2$: $10(\theta - 100)^2 + 2\theta^2 \rightarrow \\ \frac{\partial}{\partial \theta} = 20(\theta - 100) + 4\theta = 0 \rightarrow \\ \theta = 83.3$

            $|e|: \min_{\theta} \sum_{i = 1}^{12} |\theta - y_i|  = 10| \theta - 100 | + 2\theta \\
             (\theta \le 100) = \min_{\theta} 10(100 - \theta) + 2\theta \\
             = 1000 - 8\theta \rightarrow \theta = 100\\
             (\theta \ge 100) = \min_{\theta} 10(\theta - 100) + 2\theta \\
             = 12\theta - 1000 \rightarrow \theta = 100 $
            
             \item How to solve l1-norms cost functions?
             
             \begin{enumerate}
                 \item No closed form
                 \item we need to be careful with gradient descent
                 \item We need to use convex programming toolboxs (convex optimizations)
             \end{enumerate}

             \item Huber loss funct
             $$
                l_{\delta} (e) = \left\{ 
                    \begin{array}{rcl}
                        \frac{1}{2}e^2 & & |e| \le \delta \\
                        \delta |e| - \frac{{\delta}^2}{2} & & |e| \ge \delta
                    \end{array} 
                    \right.
             $$

             $$
             \frac{\partial l_{\delta}(e)}{\partial e} = 
             \left\{
                 \begin{array}{rcl}
                    e & & -\delta \le e le \delta\\
                    \delta & & e > \delta\\
                    -\delta & & e < \delta
                 \end{array}
             \right.
             $$

             in huber loss function, we don't have closed form solution but we can run gredient descent now.

             \item \defi Overfitting: 
             
             Learning a system from traning data that does very well on training data itself (e.g, very low regression error on traning data), but performs poorly on test data.

             \item \defi Overfitting in Linear Regression
             
             $\Phi^T \Phi \theta = \Phi^T Y \\ 
             \Rightarrow \theta^* = (\Phi^T \Phi)^{-1} \Phi^T Y$

             rank$(\Phi^T \Phi) \le \min \{rk(\Phi^T), rk(\Phi)\} = rk(\Phi) \le \min\{N,d\}$

             $\Phi^T\Phi$ is $d\times d$ matrix, then rank is $\le d$.

             Therefore, when $N < d$ it is not invertible which means we have multiple solutions and results in overfitting.
        \end{enumerate}
        \title{DS4400 Notes 01/28}
        \maketitle
        \begin{enumerate}
            \item \defi Overfitting\\
            Refers to situation where the learned model does well on traning data and poorly on testing data.\\
            As $d$ (dimension of system) increases, then training error godes down (can be exactly ZERO for sufficiently large d)

            \item In Linear regression:\\
            $$\min \sum_{i = 1}^n (\theta^T \phi(x_i) - y_i)^2$$ set the derivative to 0 and we find $$\Phi^T \Phi \theta = \Phi^T Y$$ Then $\theta^* = (\Phi^T \Phi)^{-1}\Phi^T Y$

            \textbf{When is it the case that $\Phi^T\Phi$ is not invertible?}
            
            Since $\Phi^T \Phi \in \mathbb{R}^{N\times d}$ 
            $$rk(\Phi^T \Phi) \le rk(\Phi) \le min\{N,d\}$$

            $\Phi^T \Phi \in \Rb^{d \times d}$ is invertible when $rk(\Phi^T \Phi) = d$. Therefore, when $N < d, rk(\Phi^T \Phi) = N$, $\Phi^T \Phi $ is not invertible. There will be infinitely many solutions for $\theta$.

            \textbf{Generally, need sufficient \# samples}

            \item Test overfitting.\\
            If $\Phi^T \Phi$ is not invertible,\\
            $\exists v \ne 0 , \Phi^T \Phi v = 0$\\
            $\Rightarrow \theta^* + \alpha v$ is also a solution for any $\alpha \in R$\\
            $\Phi^T \Phi (\theta^* + \alpha v) = \Phi^T \Phi \theta^* + \Phi^T \Phi (\alpha v) \\ = \Phi^T \Phi \theta^* + \alpha \Phi^T \Phi v \\ = \Phi^T \Phi \theta^* = \Phi^T Y$

            We can find large $\alpha$ so that $\theta^*$ have extremly large entries.

            \textbf{Generally, if the entries are very large (abs) we might have overfitting}

            \item Treat overfitting\\
            We want to change regreession optimization to prevent $\theta$ from very large terms.

            then we change the cost function:

            $$\min_{\theta} \sum_{i = 1}^{N}(\theta^T\phi(x_i) - y_i)^2 + \lambda \sum_{j = 1}^d \theta_j^2$$

            $\lambda$: regularization parameter $(> 0)$\\
            $\sum_{j = 1}^d \theta_j^2$: regularizer.\\      
            $\lambda \rightarrow 0$: back to overfitting \\
            $\lambda \rightarrow \infty: \theta^* = 0$, underfitting
            \begin{enumerate}
                \item closed-form \\
                    $\frac{\partial J}{\partial \theta} \\
                    = 2 \Phi^T(\Phi \theta - Y) + \lambda \frac{\partial \sum_{j = 1}^N \theta_j^2}{\partial \theta} \\
                    = 2\Phi^T(\Phi \theta - Y) + 2\lambda \theta$\\
                    Let it be zero:
                    $$\Phi^T \Phi \theta + \lambda \theta = \Phi^T Y$$
                    $$(\Phi^T \Phi + \lambda I_d )\theta = \Phi^T Y$$
                    Then $\theta^* = (\Phi^T \Phi + \lambda I_d )^{-1}\Phi^T Y$
                \item Gradient descent \\
                    Find initial $\theta^{(0)}$\\
                    $\theta^{t} = \theta^{(t-1)} - \rho \frac{\partial J}{\partial \theta}|_{\theta^{(t-1)}} \\
                    = \theta^{(t-1)} - 2\Phi^T(\Phi \theta^{(t-1)} - Y) + 2\lambda \theta^{(t-1)}$
            \end{enumerate}

            \item Hyperparameter Tunning
            
            GD: set learning rate $\rho$

            Robust Reg: Huber loss $\delta$

            overfitting and regularization: $\lambda$

            $\rho, \delta, \lambda = $ hyperparameters

            \textbf{How to pick hyperparameters?}

            \textbf{BAD APPROACH 1:}

            \begin{enumerate}
                \item pick some set of possible $\lambda_i \in \{\lambda_1, \lambda_2 \dots\}$
                
                Run regression with $\lambda_i$ and find $\theta^*_i$

                Measure regression error:

                $$\epsilon_{tr}(\lambda) = \sum_{i=1}^N ((\theta^*(\lambda))^T x_i - y_i)^2$$

                To sum: just find $\lambda$ for which $\epsilon_{tr}(\lambda)$ is minimum
            \end{enumerate}

            \textbf{This approach is setting $\lambda$ back to 0}

            \textbf{Test data needed!!!}

            \begin{enumerate}
                \item We need to Train $\lambda_i$ on \textbf{training set} to minimize the cost function $$2\Phi^T(\Phi \theta - Y) + 2\lambda \theta$$ to find $\theta^*_i$
                \item Measure regression error on the \textbf{hold-out set} $D^{ho}$
                $$\epsilon_{tr} = \sum_{x_i,y_i \in D^{ho} } (y_i - (\theta^*(\lambda))^Tx_i)^2$$

            \end{enumerate}
            
        \end{enumerate}
        \newpage
    \end{multicols}

    \newpage
    
\end{document}