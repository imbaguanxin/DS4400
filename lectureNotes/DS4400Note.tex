\documentclass[12pt]{article}  
\usepackage{graphicx}
\usepackage{geometry}   %设置页边距的宏包
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{kpfonts}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{color}
\newcommand{\solu}{{\color{blue} Solution:}}
\newcommand{\theo}{{\color{blue} Theorem: $\ $}}
\newcommand{\defi}{{\color{blue} Definition: $\ $}}
\newcommand{\recall}{{\color{blue} Recall: $\ $}}
\newcommand{\exe}{{\color{green} Exercise: $\ $}}
\newcommand{\prop}{{\color{blue} Prop: $\ $}}

\newcommand{\hw}{{\color{red} Homework: $\ $}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Lap}{\mathcal{L}}
\newcommand{\Zplus}{\mathbf{Z}^+}
\newcommand{\indep}{\perp \!\!\! \perp}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\geometry{left=2cm,right=2cm,top=1.5cm,bottom=2cm}  %设置 上、左、下、右 页边距

\title{DS4400 Notes}
\date{}
\author{}

\begin{document}
    \maketitle

    \begin{multicols}{2}
        \title{DS4400 Notes 01/24}
        \maketitle
        \begin{enumerate}
            \item Convex functions:
            
            A function $f:\mathbb{R}^d \rightarrow \mathbb{R}$ is convex iff $\forall \theta_1, \theta_2 \in \mathbb{R}^d $ and $\forall \alpha \in [0,1]$ we have 
            $f(\alpha\theta_1 + (1-\alpha)\theta_2) \le \alpha f(\theta_1) + (1-\alpha)f(\theta_2)$
    
            In the special case $(d=1) \ f:\mathbb{R}\rightarrow \mathbb{{R}}$, $f$ is convex iff $\forall \theta, f''(\theta) \ge 0$

            When the function is convex, \textbf{local min} $\equiv$ \textbf{global min}. When the system is not convex, we might find only a \textbf{local min} but not a \textbf{global min}

            \item Dealing with non convex function:
            
            In gradient descent: 
            \begin{enumerate}
                \item use larger $\rho$ in the beginning and gradually decrease $\rho$ with interation.
                \item Run SGD/GD with multiple random initializaitons $\theta_1^{(0)}, \theta_2^{(0)} \dots $ and keep the best solution.
            \end{enumerate}

            \item $\argmin_{\theta} \sum_{i = 1}^N (y_i - \theta^Tx_i)^2 \triangleq J(\theta)$ 
            
            In linear regression, $J(\theta)$ is convex.

            \item Robustness of Regression to outliers:
            
            \begin{enumerate}
                \item Run outlier detection algorithm, remove detected outliers, then run Linear Regression on remaining points.
                \item Robust Regression cost function.
                
                $\argmin_{\theta} \sum_{i = 1}^{N} e_i^2, \ e_i \triangleq y_i - \theta^T x_i$

                $e^2$ is extremly unhappy with large errors.

                we might use $|e|$ to replace the function. This might be more tolerance. Then, $\argmin_{\theta} \sum_{i = 1}^N |y_i - \theta^T x_i|$
            \end{enumerate}
            
            \item \exe D = \{$(x_1, y_1 = 100) \dots (x_10, y_10 = 100), (x_{11}, y_{11} = 0),  (x_{12}, y_{12} = 0)$\}
            
            $e^2$: $10(\theta - 100)^2 + 2\theta^2 \rightarrow \\ \frac{\partial}{\partial \theta} = 20(\theta - 100) + 4\theta = 0 \rightarrow \\ \theta = 83.3$

            $|e|: \min_{\theta} \sum_{i = 1}^{12} |\theta - y_i|  = 10| \theta - 100 | + 2\theta \\
             (\theta \le 100) = \min_{\theta} 10(100 - \theta) + 2\theta \\
             = 1000 - 8\theta \rightarrow \theta = 100\\
             (\theta \ge 100) = \min_{\theta} 10(\theta - 100) + 2\theta \\
             = 12\theta - 1000 \rightarrow \theta = 100 $
            
             \item How to solve l1-norms cost functions?
             
             \begin{enumerate}
                 \item No closed form
                 \item we need to be careful with gradient descent
                 \item We need to use convex programming toolboxs (convex optimizations)
             \end{enumerate}

             \item Huber loss funct
             $$
                l_{\delta} (e) = \left\{ 
                    \begin{array}{rcl}
                        \frac{1}{2}e^2 & & |e| \le \delta \\
                        \delta |e| - \frac{{\delta}^2}{2} & & |e| \ge \delta
                    \end{array} 
                    \right.
             $$

             $$
             \frac{\partial l_{\delta}(e)}{\partial e} = 
             \left\{
                 \begin{array}{rcl}
                    e & & -\delta \le e le \delta\\
                    \delta & & e > \delta\\
                    -\delta & & e < \delta
                 \end{array}
             \right.
             $$

             in huber loss function, we don't have closed form solution but we can run gredient descent now.

             \item \defi Overfitting: 
             
             Learning a system from traning data that does very well on training data itself (e.g, very low regression error on traning data), but performs poorly on test data.

             \item \defi Overfitting in Linear Regression
             
             $\Phi^T \Phi \theta = \Phi^T Y \\ 
             \Rightarrow \theta^* = (\Phi^T \Phi)^{-1} \Phi^T Y$

             rank$(\Phi^T \Phi) \le \min \{rk(\Phi^T), rk(\Phi)\} = rk(\Phi) \le \min\{N,d\}$

             $\Phi^T\Phi$ is $d\times d$ matrix, then rank is $\le d$.

             Therefore, when $N < d$ it is not invertible which means we have multiple solutions and results in overfitting.
        \end{enumerate}
        \title{DS4400 Notes 01/28}
        \maketitle
        \begin{enumerate}
            \item \defi Overfitting\\
            Refers to situation where the learned model does well on traning data and poorly on testing data.\\
            As $d$ (dimension of system) increases, then training error godes down (can be exactly ZERO for sufficiently large d)

            \item In Linear regression:
            
            $$\min \sum_{i = 1}^n (\theta^T \phi(x_i) - y_i)^2$$ set the derivative to 0 and we find $$\Phi^T \Phi \theta = \Phi^T Y$$ Then $\theta^* = (\Phi^T \Phi)^{-1}\Phi^T Y$

            \textbf{When is it the case that $\Phi^T\Phi$ is not invertible?}
            
            Since $\Phi^T \Phi \in \mathbb{R}^{N\times d}$ 
            $$rk(\Phi^T \Phi) \le rk(\Phi) \le min\{N,d\}$$

            $\Phi^T \Phi \in \Rb^{d \times d}$ is invertible when $rk(\Phi^T \Phi) = d$. Therefore, when $N < d, rk(\Phi^T \Phi) = N$, $\Phi^T \Phi $ is not invertible. There will be infinitely many solutions for $\theta$.

            \textbf{Generally, need sufficient \# samples}

            \item Test overfitting.\\
            If $\Phi^T \Phi$ is not invertible,\\
            $\exists v \ne 0 , \Phi^T \Phi v = 0$\\
            $\Rightarrow \theta^* + \alpha v$ is also a solution for any $\alpha \in R$\\
            $\Phi^T \Phi (\theta^* + \alpha v) = \Phi^T \Phi \theta^* + \Phi^T \Phi (\alpha v) \\ = \Phi^T \Phi \theta^* + \alpha \Phi^T \Phi v \\ = \Phi^T \Phi \theta^* = \Phi^T Y$

            We can find large $\alpha$ so that $\theta^*$ have extremly large entries.

            \textbf{Generally, if the entries are very large (abs) we might have overfitting}

            \item Treat overfitting\\
            We want to change regreession optimization to prevent $\theta$ from very large terms.

            then we change the cost function:

            $$\min_{\theta} \sum_{i = 1}^{N}(\theta^T\phi(x_i) - y_i)^2 + \lambda \sum_{j = 1}^d \theta_j^2$$

            $\lambda$: regularization parameter $(> 0)$\\
            $\sum_{j = 1}^d \theta_j^2$: regularizer.\\      
            $\lambda \rightarrow 0$: back to overfitting \\
            $\lambda \rightarrow \infty: \theta^* = 0$, underfitting
            \begin{enumerate}
                \item closed-form \\
                    $\frac{\partial J}{\partial \theta} \\
                    = 2 \Phi^T(\Phi \theta - Y) + \lambda \frac{\partial \sum_{j = 1}^N \theta_j^2}{\partial \theta} \\
                    = 2\Phi^T(\Phi \theta - Y) + 2\lambda \theta$\\
                    Let it be zero:
                    $$\Phi^T \Phi \theta + \lambda \theta = \Phi^T Y$$
                    $$(\Phi^T \Phi + \lambda I_d )\theta = \Phi^T Y$$
                    Then $\theta^* = (\Phi^T \Phi + \lambda I_d )^{-1}\Phi^T Y$
                \item Gradient descent \\
                    Find initial $\theta^{(0)}$\\
                    $\theta^{t} = \theta^{(t-1)} - \rho \frac{\partial J}{\partial \theta}|_{\theta^{(t-1)}} \\
                    = \theta^{(t-1)} - 2\Phi^T(\Phi \theta^{(t-1)} - Y) + 2\lambda \theta^{(t-1)}$
            \end{enumerate}

            \item Hyperparameter Tunning
            
            GD: set learning rate $\rho$

            Robust Reg: Huber loss $\delta$

            overfitting and regularization: $\lambda$

            $\rho, \delta, \lambda = $ hyperparameters

            \textbf{How to pick hyperparameters?}

            \textbf{BAD APPROACH 1:}

            \begin{enumerate}
                \item pick some set of possible $\lambda_i \in \{\lambda_1, \lambda_2 \dots\}$
                
                Run regression with $\lambda_i$ and find $\theta^*_i$

                Measure regression error:

                $$\epsilon_{tr}(\lambda) = \sum_{i=1}^N ((\theta^*(\lambda))^T x_i - y_i)^2$$

                To sum: just find $\lambda$ for which $\epsilon_{tr}(\lambda)$ is minimum
            \end{enumerate}

            \textbf{This approach is setting $\lambda$ back to 0}

            \textbf{Test data needed!!!}

            \begin{enumerate}
                \item We need to Train $\lambda_i$ on \textbf{training set} to minimize the cost function $$2\Phi^T(\Phi \theta - Y) + 2\lambda \theta$$ to find $\theta^*_i$
                \item Measure regression error on the \textbf{hold-out set} $D^{ho}$
                $$\epsilon_{tr} = \sum_{x_i,y_i \in D^{ho} } (y_i - (\theta^*(\lambda))^Tx_i)^2$$

            \end{enumerate}
        \end{enumerate}
        \title{DS4400 Notes 01/31}
        \maketitle

        \begin{enumerate}
            \item Hyperparameter Tunning:
            $$\min_{\theta} \sum_{i = 1}^{N}(\theta^T\phi(x_i) - y_i)^2 + \lambda \sum_{j = 1}^d \theta_j^2$$

            \begin{itemize}
                \item For $\lambda \in \{\lambda_1, \lambda_2 \dots, \lambda_p\}$
                \begin{itemize}
                    \item Tran using $D^{tr}$ with $\lambda \rightarrow \theta^*(\lambda)$
                    \item Measure validation error $$\epsilon^{tr}(\lambda) = \sum_{x_i,y_i \in D^{ho} } (y_i - (\theta^*(\lambda))^Tx_i)^2$$
                \end{itemize}
                \item select $\lambda$ which minimizes $$\epsilon^{ho}(\lambda)  \rightarrow \lambda^* =  \min_{\{\lambda_1, \lambda_2 \dots, \lambda_p\}}\epsilon^{ho}(\lambda)$$
            \end{itemize}

            \item Problems:
            \begin{itemize}
                \item Take much longer time since we are training the models multiple times
                \item Each training is using a subset of the data set, then each training is amplifing the problem of overfitting.
            \end{itemize}

            \item K-fold cross validation
            
            divide Data set to k equally large sets $\{D_1, D_2, \dots, D_k\} \in D$

            \begin{itemize}
                \item For $\lambda \in \{\lambda_1, \lambda_2 \dots, \lambda_p\}$
                \begin{itemize}
                    \item For $i = 1,2, \dots, k$
                    \begin{itemize}
                        \item train on $\bigcup\limits_{j\ne i}D^j$ and get $\theta_i^*(\lambda)$
                        \item compute validate error on $D^i \rightarrow \epsilon^{ho}_i(\lambda)$
                    \end{itemize}
                    \item compute average of $\{\epsilon_i^{ho}(\lambda)$\}: $\epsilon^{ho} = \frac{1}{k}\sum_{i = 1}^{k}\epsilon^{ho}(\lambda)$
                \end{itemize}
                \item select $\lambda^* = \min\limits_{\{\lambda_1, \lambda_2 \dots, \lambda_p\}}\epsilon^{ho}(\lambda)$
            \end{itemize}

            Once we find the best $\lambda$, train the model on the whole set.

            \item PROBABILITY REVIEW
            \begin{itemize}
                \item Random Variable: a variable that takes values corresponding to outcome of a random phenomenon.
                \item Discrete r.v.: descrete values
                \item continuous r.v. continus range of values
                \item Condition: $P(X|Y) = \frac{P(X \cap Y)}{P(Y)}$
                $$P(X,Y) = P(X|Y)P(Y)$$
                $$P(X,Y) = P(Y|X)P(X)$$
                \textbf{Chain rule:} 
                
                $P(X_1, X_2, \dots, X_n) = P(X_1)P(X_2|X_2)P(X_3|X_1,X_2)\\ \dots P(X_N|X_1,X_2 \dots X_N) $

                \item Marginalization
                
                $p(x,y)$ known\\
                $p(x) = \sum\limits_y p(x, Y= y) = \int p(x,y)dy$  

                \item Bayes Rule:\\
                $P(X|Y) = \frac{P(Y|X)P(X)}{P(Y)} = \frac{P(X|Y)P(Y)}{P(X)} $

                \item Independence: \\
                r.v. are independent ($X\indep Y$) iff \\
                $P(X|Y) = p(X), P(Y|X) = p(Y)$
                \\or $P(x,y) =P(x)p(y)$
                \item conditional independence
                example: X = height of person, Y = vocabulary, X is not independent of Y since babies may have less vocabulary and with lower heights.\\
                However, X = height, Y = vocab, Z = age. Then $(X\indep Y)\ | \ Z$
                $$P(X,Y|Z) = P(X|Z)P(Y|Z)$$
                $$\Rightarrow P(X|Y, Z) = P(X|Z)$$

                \item Expectation:\\
                $E(X) = \sum xp(x) $ or $\int xp(x)dx$\\
                $E(f(X)) = \sum f(x)p(x)$ or $\int f(x)p(x)dx$\\
                Given $X\indep Y$, $E[XY] = E[X]E[Y] $ \\
                hint: $(E[XY] = E[f(x,y)]$

                \item IID r.v: independent and identically destributed\\
                $p(X_1= x_1, X_2 = x_2, \dots X_n = x_n) = p(X_1 = x_1)p(X_2 = x_2)\dots p(X_n = x_n)$ and each expriment is identical.\\ $P(X_1 = \theta) = P(X_2 = \theta) = \dots = P(X_n = \theta)$
            \end{itemize}
        \end{enumerate}

        \title{DS4400 Notes 02/04}
        \maketitle

        \textbf{Maximum Likelihood Estimation}
        \begin{enumerate}            
            \item Some distributions:
            \begin{itemize}
                \item Gaussian Dist.
                
                $P(X=x) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
                \item Laplace Dist.
                
                $P(X = x) = \frac{1}{2\lambda}e^{-\frac{|x-\mu|}{\lambda}}$

                \item Bernoulli Dist.
                
                $P_\theta(x = 1) = \theta, P_\theta(x = 0) = 1 -\theta$
            \end{itemize}

            \item Goal: Learn parameters of probability models. (fix the prob. model class)
            
            In ML, we learn these parameters ($\theta$) using training data, D.

            We want to measure $P_\theta(D)$

            MLE: $\theta^* = \argmax_{\theta}P_\theta(D)$ Under such $\theta^*$, the probability of observing the given dataset is maximum.

            \item \exe Fliping a coin.
            
            This is a Binomial Dist. (n time Bernoulli Trials)
            
            model: $p(X = x) = \theta^{x}(1-\theta)^{1-x}, x = 0, 1$

            $P_\theta(D) = P_\theta(X_1 = x_1, X_2 = x_2 \dots)$

            Assuming that tossing coins are iids:

            $P_\theta(D) = P_\theta(x_1)P_\theta(x_2) \dots\\
            = \theta^{\sum x_i} (1-\theta)^{\sum (1-x_i)}$

            Then likelihood funciton: 

            $L(\theta) = P_\theta(D)$

            Take the logrithm of both sides (simplify product to sum)

            $\theta^* = \argmax_\theta logL(\theta)$

            \textbf{REASON:} 
            
            1. log is monotonically increasing
            
            2. simplify the powers to scale, the product to sum.

            3. Increase the dynamic range (working with small numbers is not accurate on computers and memory consuming)

            $$\frac{\partial log L(\theta)}{\partial \theta} = \sum x_i \frac{1}{\theta} + (N - \sum x_i) \frac{-1}{1-\theta} $$

            Let the derivative equals to 0.

            $$\frac{1}{\theta}\sum x_i = \frac{1}{\theta -1}(N - \sum x_i)$$
            $$\theta = \frac{\sum x_i}{N}$$

            \item \exe People's height
            
            Use model: normal distribution.

            $L(\theta)= P_{\sigma, \mu}(D) = \prod \limits_{i = 1}^{N} P_e(x_i)$

            $logL(\theta) = -\frac{N}{2}log(2\pi\sigma^2) - \frac{\sum\limits_{i = 1}^{N}(x_i-\mu)^2}{2\sigma^2} = -\frac{N}{2}log(2\pi) -\frac{N}{2}log(\sigma^2) - \frac{\sum\limits_{i = 1}^{N}(x_i-\mu)^2}{2\sigma^2}$

            $$\frac{\partial logL(\theta)}{\partial \mu} = -\sum_{i = 1}^N (\mu - x_i)/ \sigma^2$$
            $$\Rightarrow \hat{\mu} =  \frac{\sum\limits_{i = 1}^{N} x_i}{N}$$
            $$\frac{\partial log L(\theta)}{\partial \sigma^2} =-\frac{N}{2}\frac{1}{\sigma^2} - \frac{\sum\limits_{i = 1}^{N}(x_i-\mu)^2}{2} \frac{-1}{\sigma^4}$$
            $$= \frac{-N}{2\sigma^2} + \frac{\sum\limits_{i = 1}^{N}(x_i-\mu)^2}{2\sigma^4}$$
            $$\hat{\sigma^{2}} = \frac{1}{N}\sum_{i = 1}^{N}(x_i - \hat{\mu})^2$$

        \end{enumerate}
        \title{DS4400 Notes 02/07}
        \maketitle\\
        Classification:\\
        \textbf{Binary Classification}:\\
         input = Email $\rightarrow$ output = 'span' vs 'non-span' \\
        \textbf{Multiclass Classification}:\\
        input = Image $\rightarrow$ output = 'car', 'bike', 'stop-sign', $\dots$\\
        \begin{itemize}
            \item \textbf{Classification Setup}:\\
            Given a training dataset $D=\{(x_1, y_1) \cdots (x_n, y_n)\}$ where $x_i \in \mathbb{R}^d$ is input feature vector and $y_i\in \{0,1,2,\dots, L-1\}$, Find a mapping $g:\mathbb{R}^d \rightarrow \{0,1,2,\dots, L-1\}$ s.t. $g_w(x_i) = y_i$ for many i's $\in \{1,2,\dots, N\}$ 
            \item \textbf{Assumption:}\\
            Assume that there is a hyperplane $w^T\phi(x) =0$ that separates data into two classes. \\
            Then set: $w^T\phi(x) > 0 \rightarrow y = 1$, set: $w^T\phi(x) < 0 \rightarrow y = 0$

            \item Logistic Regression Model:\\
            $P_w(y = 1 | x) \propto e^{w^T\phi(x)/2}$\\
            $P_w(y = 0 | x) \propto e^{-w^T\phi(x)/2}$\\
            Determine Z: $P_w(y = 1 | x) + P_w(y = 0 | x) = 1$:\\
            $\frac{1}{z}e^{w^T\phi(x)/2} + \frac{1}{z} e^{-w^T\phi(x)/2} = 1$\\
            $\rightarrow z = e^{w^T\phi(x)/2} + e^{-w^T\phi(x)/2}$\\
            $\rightarrow P_w(y = 1 | x) = \frac{1}{z}e^{w^T\phi(x)/2} = \frac{1}{1 + e^{-w^T\phi(x)}}$\\

            Model: $P_w(y = 1 | x) = \frac{1}{1 + e^{-w^T\phi(x)}}$\\
            Model: $P_w(y = 0 | x) = 1- \frac{1}{1 + e^{-w^T\phi(x)}}$
            \item signoid/logistic function:\\
            $\sigma(z) = \frac{1}{1 + e^{-z}}$
            \item Logistic regression:\\
            $P_w(y = 1 | x) = \frac{1}{1 + e^{-w^T\phi(x)}} = \sigma(w^T\phi(x))$

            \item Training: Learn $w^*$ given training data D
            \item Testing: $P_w(y^n=1|x^n) = \frac{1}{1 + e^{-{w^*}^T\phi(x^n)}}$
            \item Assign $P>0.5 \rightarrow class 1$, $P\le0.5 \rightarrow class 0$
            \item Training via MLE:\\
            $\max\limits_{w}P_w(D) = \max\limits_{w}P_w(y_1 | x_1) \cdots P_w(y_N | x_N)\\
            = \max\limits_{w}\prod\limits_{i = 1}^{N}P_w(y_i | x_i)$ \\
            We can write :\\
            $P_w(y_i|x_i) = P_w(y_i = 1 | x_1)^{y_i} P_w(y_i = 0 | x_1)^{1 - y_i}$
            Apply natural log:
            $\max\limits_{w} logP_w(D) = \max\limits_{w}\sum_{i = 1}^{N}log[(\frac{1}{1 + e^{-w^T\phi(x_i)}})^{y_i} + (\frac{1}{1 + e^{w^T\phi(x_i)}})^{1 -y_i}]\\
            = \max\limits_{w}\sum_{i = 1}^{N}(y_i)log(\frac{1}{1 + e^{-w^T\phi(x_i)}}) + (1 - y_i)log(\frac{1}{1 + e^{w^T\phi(x_i)}})\\
            = \max\limits_{w}\sum_{i = 1}^{N}(y_i)[log(\frac{1}{1 + e^{-w^T\phi(x_i)}}) - log(\frac{1}{1 + e^{w^T\phi(x_i)}})] + log\frac{1}{1 + e^{w^T\phi(x_i)}}\\
            = \max\limits_{w}\sum_{i = 1}^{N}(y_i)[log(\frac{1 + e^{w^T\phi(x_i)}}{1 + e^{-w^T\phi(x_i)}})] + log\frac{1}{1 + e^{w^T\phi(x_i)}}\\
            \max\limits_{w}\sum_{i = 1}^{N}(y_i)[log(e^{w^T\phi(x_i)})] + log\frac{1}{1 + e^{w^T\phi(x_i)}}\\
            = \max\limits_{w}\sum_{i = 1}^{N}(y_iw^T\phi(x_i)) - log(1 + e^{w^T\phi(x_i)})\\
            \equiv \min\limits_{w}\sum_{i = 1}^{N}-(y_iw^T\phi(x_i)) + log(1 + e^{w^T\phi(x_i)})$\\
            Derivative:\\
            $\frac{\partial J}{\partial w}$
        \end{itemize}
        
        
        \newpage
    \end{multicols}

    \newpage
    
\end{document}