\documentclass[12pt]{article}  
\usepackage{graphicx}
\usepackage{geometry}   %设置页边距的宏包
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{kpfonts}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{color}
\usepackage{pgf,tikz}
\usetikzlibrary{automata, positioning, arrows}
\usepackage{wrapfig}
\newcommand{\solu}{{\color{blue} Solution:}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Lap}{\mathcal{L}}
\newcommand{\Zplus}{\mathbf{Z}^+}
\newcommand{\indep}{\perp \!\!\! \perp}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}  %设置 上、左、下、右 页边距

\title{DS4400 HW2}
\author{Xin Guan}
\date{}

\begin{document}
    \maketitle
    \begin{enumerate}
        \item \textbf{Linear Regression:} Consider the modified linear regression problem
        $$\hat{\theta} = \argmin_\theta \sum_{i = 1}^{N}(\mathbf{\theta}^T\phi(x_i) - y_i)^2 + \lambda||\mathbf{\theta} - \mathbf{a}||^2_2$$
        where $a$ is a known and given vector of the same dimension as that of $\theta$. Derive the closed-form solution. Provide all steps of the derivation.
        
        \solu

        $f(\theta) = \sum_{i = 1}^{N}(\mathbf{\theta}^T\phi(x_i) - y_i)^2 + \lambda||\mathbf{\theta} - \mathbf{a}||^2_2$\\
        $\frac{\partial f(\theta)}{\partial \theta} = \frac{\partial \sum_{i=1}^{N} (\theta^T\phi(x_i) -y_i)^2}{\partial \theta} + \frac{\partial \lambda ||\theta - a||^2_2}{\partial \theta}$\\
        $ = \sum_{i=1}^{N} [2(\theta^T\phi(x_i) -y_i)\frac{ \partial (\theta^T\phi(x_i) -y_i)}{\partial \theta}] +  \lambda\frac{\partial ||\theta - a||^2_2}{\partial \theta}$\\
        $ = \sum_{i=1}^{N} [2(\theta^T\phi(x_i) -y_i) \phi(x_i)] +  \lambda\frac{\partial (\theta - a)^2}{\partial \theta}$\\
        $\lambda\frac{\partial (\theta - a)^2}{\partial \theta} = \lambda \frac{\partial (\theta^2 - 2\theta a + a^2)}{\partial \theta} = \lambda (2\theta -2a)$  \\
        Therefore, $\frac{\partial f(\theta)}{\partial \theta} = \sum_{i=1}^{N} [2(\theta^T\phi(x_i) -y_i) \phi(x_i)] + 2\lambda(\theta - a)$\\
        Write all data $\phi(x_1), \phi(x_2) \dots \phi(x_N)$ as a matrix:\\
        $\Phi = \begin{bmatrix}
            \phi(x_1)^T \\ \phi(x_2)^T \\ \dots \\ \phi(x_N)^T
        \end{bmatrix}$ the dimension is $N \times d$\\
        $Y = \begin{bmatrix}
            y_1 \\  y_2 \\ \dots \\ y_n
        \end{bmatrix}$ the dimension is $N \times d$\\
        Then $\frac{\partial f(\theta)}{\partial \theta} = 2(\Phi^T\Phi\theta- \Phi^T Y) - 2\lambda(\theta - a)$\\
        Let $\frac{\partial f(\theta)}{\partial \theta} = 0$\\
        $$\Phi^T\Phi\theta- \Phi^T Y = \lambda(\theta - a)$$
        $$\Phi^T\Phi\theta- \Phi^T Y = \lambda I_d \theta - \lambda I_da$$
        $$\Phi^T\Phi\theta -  \lambda I_d\theta  = \Phi^T Y  - \lambda I_da$$
        $$(\Phi^T\Phi - \lambda I_d)\theta  = \Phi^T Y  - \lambda I_da$$
        $$\theta= (\Phi^T\Phi - \lambda I_d)^{-1}(\Phi^T Y  - \lambda I_da)$$
        Therefore, $\hat{\theta}$ is $(\Phi^T\Phi - \lambda I_d)^{-1}(\Phi^T Y  - \lambda I_da)$

        \item \textbf{Robust Regression using Huber Loss:} In the class, we defined the Huber loss as 
        $$\ell_\delta(e) = \left\{
            \begin{array}{rcl}
            \frac{1}{2}e^2 & & |e| \le \delta \\
            \delta |e| - \frac{{\delta}^2}{2} & & |e| \ge \delta
        \end{array} 
        \right.$$
        Consider the robust regression model
        $$\min_\theta \sum_{i = 1}^{N}\ell_\delta(y_i - \theta^T\phi(x_i))$$
        where $\phi(x_i)$ and $y_i$ denote the $i$-th input sample and output/response, respectivly and unknown parameter vector.\\
        a) Provide the steps of the batch gradient descent in order to obtain the solution for $\theta$.\\
        \solu \\
        Let $J(\theta) = \sum_{i = 1}^{N}\ell_\delta(y_i - \theta^T\phi(x_i))$\\
        We have : $\frac{\partial \ell_\delta(e)}{\partial e} = \left\{
            \begin{array}{rcl}
            e & & |e| \le \delta \\
            \delta & & e \ge \delta \\
            -\delta & & e \le -\delta
        \end{array} 
        \right.$\\
        Therefore,
        $\frac{\partial J(\theta)}{\partial \theta} = \frac{\sum_{i = 1}^{N}\partial \ell_\delta(y_i - \theta^T\phi(x_i))}{\partial \theta} = \sum_{i = 1}^{N}\left\{ 
            \begin{array}{rcl}
                [y_i-\theta^T\phi(x_i)] \cdot \phi(x_i)  & & |y_i-\theta^T\phi(x_i)| \le \delta \\
                \delta\cdot \phi(x_i) & & y_i-\theta^T\phi(x_i) \ge \delta \\
                -\delta\cdot \phi(x_i) & & y_i-\theta^T\phi(x_i) \le -\delta
            \end{array} 
        \right.$\\
        \textbf{Gradient Descent Steps:}\\
        Assuming we have a Maximum iteration number $T_{max}$, threshold $\epsilon$ and Learning rate $\rho$.
        \begin{enumerate}[label=(\roman*)]
            \item Pick the initial point $\theta^0$
            \item For $t = 1,2, \dots, T_{max}$
            \begin{itemize}
                \item for $i = 1,2, \dots, N$, calculate $\frac{\partial \ell_\delta(y_i - \theta^T\phi(x_i))}{\partial \theta} = \left\{ 
                    \begin{array}{rcl}
                        [y_i-\theta^T\phi(x_i)] \cdot \phi(x_i)  & & |y_i-\theta^T\phi(x_i)| \le \delta \\
                        \delta\cdot \phi(x_i) & & y_i-\theta^T\phi(x_i) \ge \delta \\
                        -\delta\cdot \phi(x_i) & & y_i-\theta^T\phi(x_i) \le -\delta
                    \end{array} 
                \right.$
                \item sum them up to get $\frac{\partial J(\theta)}{\partial \theta}$
                \item If $||\frac{\partial J(\theta)}{\partial \theta}||_2^2 \le \epsilon$, return $\theta^{t-1}$; else, $\theta^t = \theta^{t-1} - \rho\frac{\partial J(\theta)}{\partial \theta}|_{\theta^{t-1}}$
            \end{itemize}
        \end{enumerate}

        b) Provide the steps of the stochastic gradient descent using mini-batches of size 1, i.e., one sample in each mini-batch, inorder to obtain the solution for $\theta$
        
        \solu

        This step is not very different from the above process. Just add a sampling step before the calculation of $\frac{\partial J(\theta)}{\partial \theta}$. In the sampling step, just randomly pick a $x_p \in \{x_1, x_2, \dots, x_N\}$
        
        Write down as:\\
        \textbf{Stochastic Gradient Descent Steps:}\\
        Assuming we have a Maximum iteration number $T_{max}$, threshold $\epsilon$ and Learning rate $\rho$.
        \begin{enumerate}[label=(\roman*)]
            \item Pick the initial point $\theta^0$
            \item For $t = 1,2, \dots, T_{max}$
            \begin{itemize}
                \item randomly pick an $x_p \in \{x_1, x_2, \dots, x_N\}$
                \item Calculate $\frac{\partial J(\theta)}{\partial \theta} = \frac{\partial \ell_\delta(y_p - \theta^T\phi(x_p))}{\partial \theta} = \left\{ 
                    \begin{array}{rcl}
                        [y_p-\theta^T\phi(x_p)] \cdot \phi(x_p)  & & |y_p-\theta^T\phi(x_p)| \le \delta \\
                        \delta\cdot \phi(x_p) & & y_p-\theta^T\phi(x_p) \ge \delta \\
                        -\delta\cdot \phi(x_p) & & y_p-\theta^T\phi(x_p) \le \delta
                    \end{array} 
                \right.$
                \item If $||\frac{\partial J(\theta)}{\partial \theta}||_2^2 \le \epsilon$, return $\theta^{t-1}$; else, $\theta^t = \theta^{t-1} - \rho\frac{\partial J(\theta)}{\partial \theta}|_{\theta^{t-1}}$
            \end{itemize}
        \end{enumerate}


        \item \textbf{Probability and Random Variables:} State true or false. If true, prove it. If false, either prove or demonstrate by a counter example. Here $\Omega$ denotes the sample space and $A^c$ denotes the complement of the event $A$. $X$ and $Y$ denote random variables.
        \begin{enumerate}
            \item For any $A, B \subseteq \Omega$ such that $0 < P(A) < 1, P(A|B) + P(A|B^c) =1$
            
            \solu \ This is \textbf{False}
            \begin{proof}
                From the Question, $P(B) + P(B^c) = 1$.\\
                $P(A|B) = \frac{P(A\cap B)}{P(B)}$, $P(A|B^c) = \frac{P(A\cap B^c )}{P(B^c)}$\\
                Since $P(A\cap B) + P(A\cap B^c) = P(A)$ \\
                $P(A|B) +P(A|B^c) =  \frac{P(A\cap B)}{P(B)} + \frac{P(A) - P(A\cap B)}{1 - P(B)} $\\
                Then we let $P(B) = 0.5$, $P(A) = 0.4$ and $P(A\cap B) = 0.3$\\
                $P(A|B) +P(A|B^c) = \frac{0.3}{0.5} + \frac{0.4 - 0.3}{1 - 0.5} = 0.6 + 0.2 = 0.8 \ne 1$\\
                Therefore, the given term is False.
            \end{proof}
            
            \item For any $A, B \subseteq \Omega$ $P(B^c\cap (A\cup B)) + P(A^c \cup B) = 1$
            
            \solu \ This is \textbf{True}
            \begin{proof}
                $P(B^c\cap (A\cup B)) = P((B^c \cap A)\cup (B^c \cap B)) = P((B^c \cap A)\cup \emptyset) = P(B^c \cap A)$\\
                Therefore, $P(B^c\cap (A\cup B)) + P(A^c \cup B) = P(B^c \cap A) + P(A^c \cup B)$
                Since $P(A) = P(A \cap B^c) + P(A \cap B)$, We can write $P(B^c \cap A) = P(A) - P(A \cap B)$\\
                Also, we can write $P(A^c \cup B) = P(A^c) + P(B) - P(A^c \cap B)$\\
                Then $P(B^c\cap (A\cup B)) + P(A^c \cup B) \\
                = P(B^c \cap A) + P(A^c \cup B)\\
                = P(A) - P(A \cap B) + P(A^c) + P(B) - P(A^c \cap B)\\
                = P(A)+ P(A^c) + P(B) - (P(A \cap B)+ P(A^c \cap B))$\\
                Since $P(A) + P(A^c) = 1$ and $P(A \cap B) + P(A^c \cap B) = P(B)$\\
                $P(B^c\cap (A\cup B)) + P(A^c \cup B) = 1$
            \end{proof}

            \item $P(A_1, \dots, A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1})$
            
            \solu \ This is \textbf{True}
            \begin{proof}
                (By induction)\\
                \textbf{Base Case: n = 1}\\
                When $n = 1, P(A_1, \dots, A_n) = P(A_1)$, \\
                $ P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1}) = P(A_1)$\\
                Therefore, when $n = 1$, $P(A_1, \dots, A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1})$ is ture.\\
                \textbf{Inductive Steps:}\\
                \textit{Inductive Hypothesis:} \\
                $P(A_1, \dots, A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1})$ is true when $n = k$.\\
                \textit{Claim:} \\
                $P(A_1, \dots, A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1})$ is true when $n = k+1$
                \textbf{Proof of Claim:}\\
                When n = k+1, right hand side:\\
                $P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1})\\
                =P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{k-1})P(A_n|A_1,\cdots,A_{k})\\
                = P(A_1, \dots, A_k)P(A_{k+1}|A_1,\cdots,A_{k+1-1}) \\
                = P(A_1, \dots, A_k)\frac{P(A_{k+1}\cap A_1,\cdots,A_k)}{P(A_1,\cdots,A_k)}\\
                = P(A_{k+1}\cap A_1,\cdots,A_k)\\
                = P(A_1, \dots, A_{k+1})$\\
                Therefore, the claim is true.\\
                Thus, $P(A_1, \dots, A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1, A_2) \cdots P(A_n|A_1,\cdots,A_{n-1})$ is true.
            \end{proof}

            \item If $X$ and $Y$ are independent discrete random variables, then $E[XY] = E[X]E[Y]$, where $E[\cdots]$ denotes expectation.
            
            \solu
            \begin{proof}
                $X \indep Y \Rightarrow P(XY) = P(X)P(Y)$\\
                $E(XY) \\
                = \sum xyP(XY) \\
                = \sum xyP(X)P(Y)\\
                = \sum xP(X) \sum yP(Y)\\
                = E[X]E[Y]$
            \end{proof}
        \end{enumerate}

        \item \textbf{Maximum Likelihood Estimation:} Assume $X_1, X_2, \dots, X_N$ are i.i.d. random variables each taking a real value, where
        $$p_\delta(X_i = x_i) = e^{-\delta^2+\delta x_i}$$
        Here, $\delta$ is the parameter of the distribution. Assume, we observe $X_1 = x_1, X_2 = x_2, \dots, X_N = x_N$.
        \begin{enumerate}
            \item Write down the likelihood function $L(\delta)$.
            
            \solu \\
            $L(\delta) = \prod\limits_{i = 1}^{i = N}p_\delta(X_i = x_i) \\
            = \prod\limits_{i = 1}^{i = N}e^{-\delta^2+\delta x_i} $

            \item Derive the maximum likelihood or log-likelihood estimation of $\delta$ for the given observations. Provide all steps of derivations.
            
            \solu \\
            $log(L(\delta)) = log(\prod\limits_{i = 1}^{i = N} e^{-\delta^2+\delta x_i}) \\
            = \sum\limits_{i = 1}^{i = N} log(e^{-\delta^2+\delta x_i})\\
            = \sum\limits_{i = 1}^{i = N} -\delta^2+\delta x_i\\
            = -N\delta^2 + \delta\sum\limits_{i = 1}^{i = N}x_i$\\
            Then $\frac{\partial log(L(\delta))}{\partial \delta} 
            = \frac{\partial( -N\delta^2 + \delta\sum\limits_{i = 1}^{i = N}x_i)}{\partial \delta}
            = -2N\delta + \sum\limits_{i = 1}^{i = N}x_i$\\
            Let $\frac{\partial log(L(\delta))}{\partial \delta} = 0$.
            Then, $-2N\delta + \sum\limits_{i = 1}^{i = N}x_i = 0\\
            \Rightarrow 2N\delta = \sum\limits_{i = 1}^{i = N}x_i
            \Rightarrow \delta = \frac{\sum\limits_{i = 1}^{i = N}x_i}{2N}$\\
            Therefore, $\hat{\delta} = \frac{\sum\limits_{i = 1}^{i = N}x_i}{2N}$
        \end{enumerate}

        \item \textbf{Logistic Regression:} In the logistic regression for binary classification $(y \in \{0,1\})$, we defined $p(y = 1 | x) = \sigma(\omega^Tx)$, where the sigmoid function is defined as
        $$\sigma(z) \triangleq \frac{1}{1 + e^{-z}}$$
        Assume we have trained the logistic regression model using a given dataset and have learned $\omega$. Let $x_n$ be a test sample.
        \begin{enumerate}
            \item Assume $\omega^T x_n < 0.3$. To which class $x_n$ belongs? Provide details of your derivations.
            
            \solu\\
            $\omega^T x_n < 0.3 \Rightarrow e^{-\omega^T x_n} > e^{-0.3}\\
            \Rightarrow 1 + e^{-\omega^T x_n} > 1 + e^{-0.3}\\
            \Rightarrow \frac{1}{1 + e^{-\omega^T x_n}} < \frac{1}{1 + e^{-0.3}}\\
            \Rightarrow p(y = 1 | x) < 0.5744$

            \item Assume $\frac{1}{1+e^{\omega^T x_n}} = 0.7.$ To which class $x_n$ belongs and with what probability? Provide detaisl of your derivations.
            
            \solu \\
            $\frac{1}{1+e^{\omega^T x_n}} = 0.7 \Rightarrow \frac{1}{0.7} = 1 + 1+e^{\omega^T x_n} \Rightarrow e^{\omega^T x_n} = \frac{1}{0.7} -1 \Rightarrow e^{-\omega^T x_n} = \frac{1}{\frac{1}{0.7} -1} = \frac{0.7}{1-0.7} = \frac{0.7}{0.3} $\\
            $\sigma(\omega^Tx_n) = \frac{1}{1+\frac{0.7}{0.3}} = \frac{0.3}{0.3+0.7} = 0.3$\\
            Therefore, $p(y = 0 | x_n) = 1 - p(y = 1 | x_n) = 1-0.3 = 0.7$ \\
            Thus, $x_n$ belongs to 0 with probability of 0.7
        \end{enumerate}
    \end{enumerate}
\end{document}